\documentclass[a4paper,english,12pt]{article}
\input{../header}
\title{Lecture-03 : Review of Linear Algebra and Convex Optimization}
\date{Aug 09, 2018}
\author{Amritendu}


\begin{document}
\maketitle
\section{Review of Linear Algebra}
\subsection{Vector Space}
A vector space over the field $\R$ is a set $V$ equipped with following two operations, each satisfying four axioms. 
\subsubsection{Vector addition}
Vector addition is a mapping $+: V \times V \to V$ defined by $+(v,w) = v+w$ for any two elements $v,w \in V$, 
that satisfies the following four axioms. 
\begin{enumerate}
\item Associativity of  addition : $u + (v + w) = (u + v) + w; $ for all $ u,v,w \in V$
\item Commutativity of addition : $u + v = v + u;  $ for all $ u,v \in V$
\item Existence of Identity: There exists a zero vector ($0 \in V$) s.t, $u+0=u; $ for all $ u\in V$
\item Existence of Inverse: For every $u \in V,$ there exists an element $-u\in V$; s.t, $u+(-u)=0$\
\end{enumerate}
\subsubsection{Scalar Multiplication}
Scalar multiplication is a mapping $\cdot : \R \times V \to V$ defined by $\cdot(\alpha, v) = \alpha v \in V$, 
that satisfies the following four axioms. 
\begin{enumerate}
\item Compatibility with the field: $a(bu)=(ab)u; $ for all $ a,b \in \R$ and $u\in V$
\item Existence of Identity : For multiplicative identity element $1 \in \R$, $1u=u; $ for all $ u\in V$
\item Distributivity over vector addition : $\alpha(vu)=\alpha u + \alpha v; $ for all $ \alpha \in \R$ and $u,v \in V$ 
\item Distributivity over field addition : $(\alpha+\beta)u=\alpha u + \beta u; $ for all $ \alpha,\beta \in \R$ and $u \in V$ 
\end{enumerate}

\begin{exmp}[Vector space] 
Following are some common examples of vector spaces.  
\begin{enumerate}
\item Space of all real numbers $\R$. 
\item Euclidean space of $N$-dimensions, denoted by $\R^N$.
\item Space of continuous functions over a compact subset $[a,b]$ denoted by $C([a,b])$. 
\end{enumerate}
\end{exmp}

\subsection{Inner Product Space}
A \textit{inner product space} is a vector space equipped with an inner product denoted by $\langle \cdot , \cdot \rangle:V \times V \to \R$ that satisfies the following axioms. 
\begin{enumerate}
\item  \textbf{Symmetry:} $\langle x,y\rangle = \langle y,x\rangle$
\item  \textbf{Linearity:} $\langle\alpha x + \beta y, z\rangle = \alpha\langle x,z\rangle +\beta\langle y,z\rangle$
\item  \textbf{Definiteness:} $\langle x,x\rangle\geq 0$; $\langle x,x \rangle = 0$ iff $x=0$
\end{enumerate}
\begin{exmp}[inner product spaces] 
Following are some common examples of inner product spaces. 
\begin{enumerate}
\item  For the vector space $V =  \R^N$, 
we can define  the inner product between two $N$-dimensional vectors as 
\EQ{
\langle x,y\rangle =\langle\begin{bmatrix}x_{1} , \dots, x_{N}\end{bmatrix}^T, \begin{bmatrix}y_{1} , \dots, y_{N}\end{bmatrix}^{T}\rangle.
 = x^{T}y = \sum_{i}^{N}x_{i}y_{i}. 
}
\item For vector space $V = C(\R^N)$, 
we can define the inner product of two continuous functions over $\R^N$ as 
\EQ{
\langle f,g\rangle  = \int_{\R^N}(f,g)(t)dt.
}
\item For the vector space of random variables, 
we can define the inner product of two random variables as 
\EQ{
\langle X,Y\rangle = \E(XY). 
}
\end{enumerate}
\end{exmp}

\subsection{Norms}
Norm is a mapping $\norm{\cdot}:V \to \R_+$ that satisfy the following axioms. 
\begin{enumerate}
\item \textbf{Definiteness:} $\norm{v} = 0$ iff  $v=0$
\item  \textbf{Homogeneity:} $\norm{\alpha v} = \abs{\alpha}\norm{v}$
\item  \textbf{Triangle inequality:} $\norm{v+w}\leq\norm{v}+\norm{w}$
\end{enumerate}
\begin{exmp}[Norms] 
Following are examples of commonly defined norms on some example vector spaces. 
\begin{enumerate}
\item $V = \R;\norm{X} = \abs{X}$
\item $V = \R^N;  \norm{X}_p = \Big(\sum_{i=1}^{N}\abs {X_{i}}^p\Big)^\frac{1}{p}$
\item $V = \R^N; \norm{X}_2 = \Big(\sum_{i=1}^{N}\abs {X_{i}}^2\Big)^\frac{1}{2}$
\end{enumerate}
\end{exmp}

\begin{prop}[Holder's Inequality]
Let p,q $\geq 1$ be conjugate, i.e.  $\frac{1}{p} + \frac{1}{q} =1 $
Then, 
\EQ{
\abs{\langle x,y\rangle} \leq \norm{x}_p \norm{y}_p \text{  for all } x,y \in \R^N.
} 
\end{prop}
\proof{
For any positive $a, b \in \R$ and conjugate pair $p,q \ge 1$ such that $1/p + 1/q = 1$, 
we have from the concavity of log
\EQ{
\ln\left(\frac{1}{p}a^p + \frac{1}{q}b^p\right) \geq \frac{1}{p}\ln a^p + \frac{1}{q}\ln b^p = \ln ab.
}
Since $\ln(\cdot)$ is an increasing function, the above inequality implies the Young's inequality $\frac{1}{p}a^p + \frac{1}{q}b^p\geq ab$. 

The Holder's inequality is trivially true if $x = 0$ or $y = 0$. 
Hence, we assume that $\norm{x}\norm{y} > 0$, and let $a = \frac{\abs {x_{i}}}{\norm x_{p}}$ and $b = \frac{\abs y_{i}}{\norm y_{p}}$. 
From Young's inequality, we have  
\EQ{
\frac{\abs x_{i}}{p\norm x_{p}^p} + \frac{\abs y_{i}}{q\norm y_{q}^q} \ge \frac{\abs x_i\abs y_i}{\norm{x}_p\norm{y}_q}, \text{  for all } i \in [N]. 
} 
Since $\abs{\langle x,y \rangle} \le \sum_{i=1}^N\abs{x_i}\abs{y_i}$, we get the result by summing both sides over $i \in [N]$ in the above inequality. 
% we get
%\EQ{
%1 \geq \sum_{i=1}^{N} \frac{\abs x_i\abs y_i}{\norm{x}_p\norm{y}_q}  \ge \frac{\displaystyle{\sum_{i=1}^{N} \abs x_i\abs y_i}}{\norm{x}_p\norm{y}_q} \geq \frac{\abs{\langle x,y \rangle}}{\norm{x}_p\norm{y}_q}.
%}

\section{Review of Convex Optimization}

Let $f:\R^N\to\R$ be a function over $N$-dimensional reals. 
Then, we can write its Taylor series expansion around the neighborhood of $x \in \R^N$ as  
\EQ{
f(y) = f(x) + \sum_{i=1}^{N} \frac{\partial f}{\partial x_i}(y_i - x_j) + \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\frac{\partial^2 f}{\partial x_i \partial x_j}(y_i - x_i)(y_j - x_j) + o (\norm{y-x}^2_2).
}
%&= f(x) + \left\langle\begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\  \frac{\partial f}{\partial x_N}\end{bmatrix}, \begin{bmatrix} {y_1 - x_1} \\ \vdots \\  y_{N} - x_{N}\end{bmatrix}\right\rangle 
%+   \begin{bmatrix} y_1 - x_1,& \hdots, & y_{N} - x_{N}\end{bmatrix}\begin{bmatrix}\frac{\partial^2 f}{\partial x_1\partial x_1} & \hdots & \frac{\partial^2 f}{\partial x_1\partial x_N}\\ \vdots & \ddots & \vdots \\  \frac{\partial^2 f}{\partial x_N\partial x_1}& \hdots & \frac{\partial^2 f}{\partial x_N\partial x_N}\end{bmatrix}\begin{bmatrix} y_1 - x_1 \\ \vdots \\  y_{N} - x_{N}\end{bmatrix}.
%}
We can define the gradient vector $\nabla f = \begin{bmatrix}\frac{\partial f}{\partial x_1} & \hdots, & \frac{\partial f}{\partial x_N}\end{bmatrix}^T$, 
and the Hessian $\nabla^2f \in \R^{N \times N}$ such that $ [\nabla^2f]_{ij}= \frac{\partial^2 f}{\partial x_i\partial x_j}$, to observe 
\EQ{
f(y) = f(x) + \nabla f^T(y-x) + (y-x)^T\nabla^2f(y-x) + o(\norm{y-x}^2_2).
}
	
\subsection{Convex Function}
Let $\sX \subseteq \R^N$. 
For a function $f: \sX \to \R$, we define its epigraph as 
\EQ{
Epi(f) \triangleq \{(x,y) \in \R^N \times \R: y \ge f(x)\}.
}

\begin{defn}
A function $f:\R^N\to\R$ is convex if it's dom$(f)$ is convex and $Epi(f)$ is convex. 
\end{defn}
\begin{note}
For a convex function $f(\cdot)$ ; $f(\alpha x + \bar{\alpha y} ) \leq \alpha f(x) + \bar{\alpha} f(y)$ where $\alpha + \bar{\alpha} =1$.\\	
	\begin{itemize}
		\item If $f$ is differentiable then $f$ is convex iff 
		\begin{enumerate}
			\item dom$(f)$ is convex
			\item $f(y) -f(x) \geq \langle\nabla f(x), y-x\rangle; $ for all $ x,y \in$ dom($f$)
		\end{enumerate}	 
		\textbf{Proof} : $ f(y) - f(x) = \langle\nabla f(x) , y-x\rangle + \frac{1}{2}(y-x)^T \nabla^2 f(x) (y-x) \geq \langle\nabla f(x) , y-x\rangle$.
		\item If $f$ is twice differentiable then $f$ is convex iff dom$(f)$ is convex and it's Hessian is positive semi definite : $\nabla^2 f(x)\succeq 0; $ for all $ x \in$ dom$(f)$
	\end{itemize}
	
\end{note}

\begin{exmp}{Convex Function}
	\begin{enumerate}
		\item Linear Function:  $f(x) = \langle w,x\rangle$; where $f:\R^N\to\R$
		\item Quadratic Function: $f(x) = x^TAx$; where A is positive semi definite 
		\item Abs Maximum $f(x) = \max \abs X_{i \in N} = \norm{X}_\infty$		
	\end{enumerate}

\end{exmp}

\begin{lem}{Composition of Functions}
	\\Let, $h(.):\R\to\R;g(.):\R^N\to\R$ and $f:\R^N\to\R$ ;  for all $ x \in \R^N$ where $f(x)$ is defined by $f(x) = h(g(x))$, then following inequalities are valid
	\begin{enumerate}
		\item If h is a convex and non decreasing and g is convex,$\Longrightarrow f(.)$ is convex\\
		\textbf{Proof}: As $g(.)$ is convex :  $g(\alpha x + \bar{\alpha}y )\leq \alpha g(x) + \bar{\alpha} g(y)$\\		
		Now, $h(g(\alpha x + \bar{\alpha}y ))\leq h(\alpha g(x) + \bar{\alpha} g(y))\leq \alpha h(g(x)) + \bar{\alpha} h(g(y))$(Proved.)
		\item If h is a convex and non increasing and g is concave,$\Longrightarrow f(.)$ is convex
		\item If h is a concave and non decreasing and g is concave,$\Longrightarrow f(.)$ is concave
		\item If h is a concave and non increasing and g is convex,$\Longrightarrow f(.)$ is concave
	\end{enumerate}
	
\end{lem}
\begin{thm}{\textbf{Jensen's Inequality}}
\\Let $X\in C \subset \R^N$ be a r.v with finite mean and $f:C\to \R$ is convex,\\
Then $\E[X] \in C$,   $\E[f(X)]\leq \infty$ and $f(\E[X]) \leq \E[f(X)]$	
\end{thm}

\begin{flushleft}
	\textbf{Proof}: $f(\displaystyle{\sum_{i=1}^{m}\alpha_{i}x_{i}}) \leq \sum_{_i=1}^{m}\alpha_{i}f(x_{i})$; where $\alpha_{i}$s could be interpreted as probabilities as $\alpha_{i}\geq0$ and $\displaystyle{\sum_{_i=1}^{m}\alpha_{i}}=1$
\end{flushleft}
	
\subsection{Constrained Optimization}
Let $f:\R^N\to\R$ and $g_{i}:\R^N\to\R, i \in [m]$ \\
\textbf{Principle Optimization Problem:} $\min f(x)$ s.t. $g_{i}(x)\leq 0;$ for all $ i \in [m]$
\begin{note}
	Let $p^*$ be the optimum value for the above problem.
\end{note}

\begin{defn}{\textbf{Lagrangian}}\\
	If $x \in \R^N$ and $\alpha \in \R_+^M$, then Lagrangian $\sL(x,\alpha):\R^N\times\R_+^M\to\R$ associated with the principal problem is defined as,   $\sL(x,\alpha) = f(x) + \displaystyle{\sum_{i=1}^{m}\alpha_i g_i(x)}$;
	The variables $\alpha \in \R_+^M$ are called Lagrange or Dual Variables.
\end{defn}

\begin{defn}{\textbf{Dual Function}}
\begin{flushleft}
	Dual function associated with the Principal Optimization Problem is defined as $F:\R_+^M\to\R$ defined as $F(\alpha) = \inf\sL(x,\alpha)$ where $x\in \R^N$
\end{flushleft}	
\end{defn}

\begin{rem}{Important Properties of Dual Function}\\

	\begin{enumerate}
		\item $F$ is concave in $\alpha$
		\item $F(\alpha) \leq \sL(x,\alpha)\leq f(x)$
		\item $F(\alpha)  \leq \adjustlimits \inf_{x\in \R^N} f(x) = p^*$
		\item $F(\alpha) \leq p^*$ such that $g_i(x) \leq 0$
	\end{enumerate}
	

\end{rem}
\subsubsection{Dual Problem:}
Dual Problem associated with Principal Optimization Problem is as follows \\
Max $F(\alpha)$ ; such that $\alpha \in \R_+^M$
\begin{note}
	Let $d^*$ be the optimal value of this dual problem.
\end{note}

\begin{rem}{Dual Function}
	\begin{enumerate}
		\item Dual problem is always convex.
		\item $d^* \leq p^*$
		\item $(p^* - d^*)$ is called duality gap. When $d^* = p^*$, it is known as strong duality. It holds for convex optimization problems where constraints are qualifying. 
	\end{enumerate}
	
\end{rem}

\begin{defn}{\textbf{Strong Constraint Qualification:}}\\
	Assume that \textbf{int}$(\sX) \neq \phi$, then the strong constraint qualification or \textbf{Slater's Condition} is defined as, 
	there exists $\bar{x} \in int(\sX)$, such that $g(\bar x) < 0$
\end{defn}

\begin{defn}{\textbf{Weak Constraint Qualification:}}
	Assume that \textbf{int}$(\sX) \neq \phi$, then the strong constraint qualification or \textbf{weak Slater's Condition} is defined as there exists $\bar{x} \in int(\sX):$ for all $ i \in [1,m],(g_i(\bar x) < 0)\vee(g_i(\bar x) = 0 \wedge g_i$ affine)
\end{defn}



\begin{thm}{Saddle Point: Sufficient Condition}\\
Let P be a constrained optimum problem over $\sX = \R^N$ If $(x^*,\alpha^*)$ is a saddle point of the associated Lagrangian, i.e. for all $ x\in \R^N, $ for all $\alpha\geq0,$
$\sL(x^*,\alpha)) \leq \sL(x^*,\alpha^*)\leq \sL(x,\alpha^*)$
Then, $(x^*,\alpha^*)$ is a saddle point of P.
\end{thm}

\begin{thm}{Saddle point-Necessary Condition}
	\begin{itemize}
		\item Assume that $f$ and $g_i$, $i\in [1,m]$ are  convex functions and Slater's condition holds, then if x is a solution of the constrained optimization problem, then there exists $\alpha\geq 0$ s.t $(x,\alpha)$ is a saddle point of the Lagrangian.
		\item Assume that $f$ and $g_i$, $i\in [1,m]$ are  convex differentiable functions and weak Slater's condition holds, then if x is a solution of the constrained optimization problem, then there exists $\alpha\geq 0$ s.t $(x,\alpha)$ is a saddle point of the Lagrangian.
	\end{itemize}
\end{thm}
\begin{thm}{Karush-Kuhn-Tucker's Theorem}\\
	Let $f,g_i : \sX\to\R,$ for all $ i\in[1,m]$ are convex and differentiable function and that the constrains are qualified. Then $\bar{x}$ is a solution of the constrained problem iff there exists $ \bar{\alpha}\geq0$ s.t.
	\begin{itemize}
	\item $\nabla_x \sL(\bar{x},\bar{\alpha}) = \nabla_xf(\bar{x}) + \langle\bar{\alpha},\nabla_x g(\bar{x})\rangle =0 $
	\item $\nabla_\alpha \sL(\bar{x},\bar{\alpha}) = g(\bar{x}) \leq 0$
	\item $\langle\bar{\alpha},g(\bar{x})\rangle =0 $
	\end{itemize} 

\end{thm}




\end{document}